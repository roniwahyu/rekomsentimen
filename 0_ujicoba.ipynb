{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m itemgetter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "from lxml import objectify\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#For blog pages\n",
    "res_rest = []\n",
    "for i in range(13):\n",
    "    print(i)\n",
    "    url = 'https://online.datasciencedojo.com/blogs/?blogpage='+str(i)\n",
    "    print(url)\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "\n",
    "    urls_temp_rest = []\n",
    "    urls_rest=[]\n",
    "    temp_rest=[]\n",
    "#     x='blogs'\n",
    "    for h in soup.find_all('a'):\n",
    "    #     print(h)\n",
    "        a = h.get('href')\n",
    "        urls_temp_rest.append(a)\n",
    "    for i in urls_temp_rest:\n",
    "        if i != None :  \n",
    "            if 'blogs' in i:\n",
    "                if 'blogpage' in i:\n",
    "                    None\n",
    "                else:\n",
    "                    if 'auth' in i:\n",
    "                        None\n",
    "                    else:\n",
    "                        urls_rest.append(i)\n",
    "    [temp_rest.append(x) for x in urls_rest if x not in temp_rest]\n",
    "    for i in temp:\n",
    "        if i=='https://online.datasciencedojo.com/blogs/':\n",
    "            None\n",
    "        else:\n",
    "            res_rest.append(i)\n",
    "    print(res_rest)\n",
    "    print('--------')\n",
    "    \n",
    "    \n",
    "    \n",
    "#Getting name and description\n",
    "name=[]\n",
    "des_temp=[]\n",
    "for j in res_rest:\n",
    "    url = j\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "\n",
    "    metas = soup.find_all('meta')\n",
    "    name.append([ meta.attrs['content'] for meta in metas if 'property' in meta.attrs and meta.attrs['property'] == 'og:title' ])\n",
    "    des_temp.append([ meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description' ])\n",
    "    \n",
    "    \n",
    "#Removing stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "descrip=[]\n",
    "# des_temp=np.array(des_temp)\n",
    "# print(des_temp)\n",
    "for i in descrip_temp:\n",
    "    for j in i:\n",
    "        text = re.sub(\"@\\S+\", \"\", j)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(\"\\$\", \"\", text)\n",
    "        text = re.sub(\"@\\S+\", \"\", text)\n",
    "        text = text.lower()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])    \n",
    "    descrip.append(text)\n",
    "\n",
    "    \n",
    "    #Building BOW\n",
    "model = Tokenizer()\n",
    "model.fit_on_texts(descrip)\n",
    "rep = model.texts_to_matrix(descrip, mode='count')\n",
    "# print(f'Key : {list(model.word_index.keys())}')\n",
    "rep_name=f'Key : {list(model.word_index.keys())}'\n",
    "\n",
    "\n",
    "#Creating df\n",
    "# name=np.array(name)\n",
    "# name=name.astype(str)\n",
    "df_name=pd.DataFrame(name)\n",
    "df_name.rename(columns = {0:'name'}, inplace = True)\n",
    "df_count=pd.DataFrame(rep)\n",
    "frames=[df_name,df_count]\n",
    "result=pd.concat(frames,axis=1)\n",
    "result=result.set_index('name')\n",
    "result=result.drop([0], axis=1)\n",
    "for i in range(len(rep)):\n",
    "    result.rename(columns = {i+1:i}, inplace = True)\n",
    "\n",
    "    \n",
    "#Calculating cosine similarity\n",
    "df_name=df_name.convert_dtypes(str)\n",
    "a=df_name['name']\n",
    "sim_df = pd.DataFrame(cosine_similarity(result, dense_output=True))\n",
    "for i in range(len(name)):\n",
    "    sim_df.rename(columns = {i:a[i]},index={i:a[i]}, inplace = True)\n",
    "sim_df\n",
    "\n",
    "\n",
    "max_val = sim_df.apply(lambda x: pd.Series(np.concatenate([x.nlargest(11).index.values])), axis=1)\n",
    "max_val\n",
    "max_val.to_csv('E:/DSD/Internal Analytics/Blogs/Suggested Blogs.csv',index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
